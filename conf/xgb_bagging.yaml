---
# this is a very minimal config file in YAML format
# it will be processed by Hydra at runtime
# you might notice it doesn't have anything special that other YAML files don't have
# check the followup tutorial on how to use Hydra in conjunction with Flower for a 
# much more advanced usage of Hydra configs

model: "XGB" #XGBoost
num_rounds: 10 # number of FL rounds in the experiment
num_clients: 50 # number of total clients available (this is also the number of partitions we need to create)
num_classes: 10 # number of classes in our dataset (we use MNIST) -- this tells the model how to setup its output fully-connected layer
num_clients_per_round_fit: 10 # number of clients to involve in each fit round (fit  round = clients receive the model from the server and do local training)
num_clients_per_round_eval: 10 # number of clients to involve in each evaluate round (evaluate round = client only evaluate the model sent by the server on their local dataset without training it)
max_attack_ratio: 0.5 # % of clients that are malicious
attack_round: "END" # FULL, MID, END are the options
attack_type: "GAN" #LF, TLF, GAN
train_method: "bagging" #
config_fit: # a config that each client will receive (this is send by the server) when they are sampled. This allows you to dynamically configure the training on the client side as the simulation progresses
  local_epochs: 60 # number of training epochs each clients does in a fit() round
  eta: 0.08
  max_depth: 6
  subsample: 0.8
  colsample_bytree: 0.8
  objective: "multi:softmax"
  eval_metric: "mlogloss"
  alpha: 8
  Lambda: 2
  tree_method: "hist"
  device: "cuda"
